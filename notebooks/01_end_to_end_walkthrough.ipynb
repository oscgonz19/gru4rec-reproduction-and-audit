{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU4Rec Reproduction Study: End-to-End Walkthrough\n",
    "\n",
    "**A Technical Deep-Dive into Session-Based Recommendations**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is a **learning artifact** designed to help you understand, modify, and experiment with the entire GRU4Rec reproduction pipeline. By the end, you'll understand:\n",
    "\n",
    "- Why session-based recommendations matter\n",
    "- How to build reproducible ML evaluation pipelines\n",
    "- The importance of proper baselines and evaluation protocols\n",
    "- How to extend and modify this system for your own research\n",
    "\n",
    "**Runtime:** ~2 minutes on CPU with synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "### 1.1 The Problem: Anonymous User Recommendations\n",
    "\n",
    "Traditional recommender systems (Netflix, Amazon) work great when you know the user. But what about:\n",
    "\n",
    "- **E-commerce visitors** who haven't logged in?\n",
    "- **News readers** browsing anonymously?\n",
    "- **First-time users** with no history?\n",
    "\n",
    "**Statistics show:**\n",
    "- 70-80% of e-commerce visitors are anonymous\n",
    "- Anonymous users convert at 1-2% vs 3-5% for returning users\n",
    "- This represents 20-40% potential revenue loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Solution: Session-Based Recommendations\n",
    "\n",
    "Instead of user history, we use **session history**:\n",
    "\n",
    "```\n",
    "Traditional:     User Profile (weeks/months of data) → Recommendations\n",
    "Session-Based:   Current Session (minutes of clicks) → Recommendations\n",
    "```\n",
    "\n",
    "**GRU4Rec** (Hidasi et al., 2016) pioneered using Recurrent Neural Networks for this task, treating each session as a sequence and predicting the next item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 What This Project Delivers\n",
    "\n",
    "| Output | Description |\n",
    "|--------|-------------|\n",
    "| **Reproducible Pipeline** | One-command execution from data to results |\n",
    "| **Baseline Models** | Popularity and Markov Chain for comparison |\n",
    "| **Evaluation Framework** | Full-ranking metrics (Recall@K, MRR@K) |\n",
    "| **Visualizations** | 11 publication-quality figures |\n",
    "| **Documentation** | Bilingual technical and executive docs |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Project Structure & Mental Model\n",
    "\n",
    "### 2.1 Repository Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the project structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root (works from notebooks/ directory)\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent if Path(os.getcwd()).name == 'notebooks' else Path(os.getcwd())\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"\\nKey directories:\")\n",
    "for item in sorted(PROJECT_ROOT.iterdir()):\n",
    "    if item.name.startswith('.') or item.name == '__pycache__':\n",
    "        continue\n",
    "    icon = \"[DIR]\" if item.is_dir() else \"[FILE]\"\n",
    "    print(f\"  {icon} {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understanding the Pipeline Stages\n",
    "\n",
    "The pipeline follows this flow:\n",
    "\n",
    "```\n",
    "┌─────────┐    ┌────────────┐    ┌─────────┐    ┌──────────┐    ┌──────────┐\n",
    "│  FETCH  │───▶│  GENERATE  │───▶│  SPLIT  │───▶│  TRAIN   │───▶│ EVALUATE │\n",
    "│ GRU4Rec │    │   Data     │    │ Temporal│    │  Models  │    │  Metrics │\n",
    "└─────────┘    └────────────┘    └─────────┘    └──────────┘    └──────────┘\n",
    "     │               │                │               │               │\n",
    "     ▼               ▼                ▼               ▼               ▼\n",
    "  vendor/     synth_sessions.tsv   train.tsv    model.pt      results.json\n",
    "                                   test.tsv     baselines      figures/\n",
    "```\n",
    "\n",
    "**Key Design Decisions:**\n",
    "1. **Fetch on-demand**: Official GRU4Rec is not redistributed (licensing)\n",
    "2. **Temporal split**: No data leakage - earlier sessions for training, later for testing\n",
    "3. **Full ranking evaluation**: Production-realistic metrics (not inflated sampled metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Entry Points\n",
    "\n",
    "| Entry Point | Purpose |\n",
    "|-------------|----------|\n",
    "| `Makefile` | Main orchestrator - run `make help` for all commands |\n",
    "| `scripts/` | Individual pipeline steps (fetch, preprocess, train, eval) |\n",
    "| `src/` | Reusable modules (baselines, metrics, visualizations) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Setup & Configuration\n",
    "\n",
    "### 3.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src to path for importing project modules\n",
    "sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
    "sys.path.insert(0, str(PROJECT_ROOT / 'scripts'))\n",
    "\n",
    "# Project modules\n",
    "from baselines import PopularityBaseline, MarkovBaseline\n",
    "from metrics import recall_at_k, mrr_at_k, ndcg_at_k\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Configuration Parameters\n",
    "\n",
    "Here are the key parameters that control the pipeline. Understanding these is crucial for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these to experiment!\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data generation\n",
    "    'n_sessions': 1000,      # Number of sessions to generate\n",
    "    'n_items': 500,          # Unique items in catalog\n",
    "    'min_session_len': 2,    # Minimum items per session\n",
    "    'max_session_len': 20,   # Maximum items per session\n",
    "    \n",
    "    # Data split\n",
    "    'train_ratio': 0.8,      # 80% train, 20% test\n",
    "    'filter_unseen': True,   # Remove unseen items from test\n",
    "    \n",
    "    # Evaluation\n",
    "    'cutoffs': [5, 10, 20],  # K values for Recall@K and MRR@K\n",
    "    \n",
    "    # Reproducibility\n",
    "    'random_seed': 42,       # For reproducible results\n",
    "}\n",
    "\n",
    "# Set random seed globally\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Why These Defaults?\n",
    "\n",
    "| Parameter | Default | Rationale |\n",
    "|-----------|---------|----------|\n",
    "| `n_sessions=1000` | Small for fast iteration; real datasets have millions |\n",
    "| `n_items=500` | Balances complexity vs speed; production has 10K-1M items |\n",
    "| `train_ratio=0.8` | Standard ML split; temporal ordering prevents leakage |\n",
    "| `cutoffs=[5,10,20]` | Standard in RecSys literature; @20 is typical production limit |\n",
    "| `random_seed=42` | Reproducibility - same data every run |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Data Ingestion / Generation\n",
    "\n",
    "### 4.1 Why Synthetic Data?\n",
    "\n",
    "We use synthetic data for several reasons:\n",
    "1. **No licensing issues** - real session data is proprietary\n",
    "2. **Fast iteration** - small data for quick experiments\n",
    "3. **Controlled properties** - we know the ground truth\n",
    "\n",
    "The synthetic data mimics real e-commerce patterns:\n",
    "- **Power-law item popularity** (few items get most clicks)\n",
    "- **Variable session lengths** (2-20 items)\n",
    "- **Temporal structure** (realistic timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data generation function from our scripts\n",
    "from make_synth_data import generate_synthetic_sessions\n",
    "\n",
    "# Generate synthetic session data\n",
    "df = generate_synthetic_sessions(\n",
    "    n_sessions=CONFIG['n_sessions'],\n",
    "    n_items=CONFIG['n_items'],\n",
    "    min_session_len=CONFIG['min_session_len'],\n",
    "    max_session_len=CONFIG['max_session_len'],\n",
    "    seed=CONFIG['random_seed']\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(df):,} interactions\")\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"\\nSchema:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Schema Explained\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `SessionId` | int | Unique session identifier (one per user visit) |\n",
    "| `ItemId` | int | Product/item identifier that was clicked |\n",
    "| `Time` | int | Unix timestamp (seconds since 1970-01-01) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "print(\"First 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute key statistics\n",
    "n_sessions = df['SessionId'].nunique()\n",
    "n_items = df['ItemId'].nunique()\n",
    "n_interactions = len(df)\n",
    "\n",
    "session_lengths = df.groupby('SessionId').size()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total interactions:     {n_interactions:,}\")\n",
    "print(f\"Unique sessions:        {n_sessions:,}\")\n",
    "print(f\"Unique items:           {n_items:,}\")\n",
    "print(f\"Avg items/session:      {n_interactions/n_sessions:.1f}\")\n",
    "print(f\"Session length range:   {session_lengths.min()} - {session_lengths.max()}\")\n",
    "print(f\"Median session length:  {session_lengths.median():.0f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data quality issues\n",
    "print(\"SANITY CHECKS:\")\n",
    "print(f\"  Null values:          {df.isnull().sum().sum()} (should be 0)\")\n",
    "print(f\"  Duplicate rows:       {df.duplicated().sum()}\")\n",
    "print(f\"  Min session length:   {session_lengths.min()} (should be >= 2)\")\n",
    "print(f\"  ItemId range:         [{df['ItemId'].min()}, {df['ItemId'].max()}]\")\n",
    "\n",
    "# Verify timestamps are ordered within sessions\n",
    "is_sorted = df.groupby('SessionId')['Time'].apply(lambda x: x.is_monotonic_increasing).all()\n",
    "print(f\"  Time sorted/session:  {is_sorted} (should be True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Item Popularity Distribution\n",
    "\n",
    "Real e-commerce data follows a **power law**: a few items get most of the clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze item popularity\n",
    "item_counts = df['ItemId'].value_counts()\n",
    "\n",
    "print(\"Item Popularity Distribution:\")\n",
    "print(f\"  Most popular item:  {item_counts.iloc[0]:,} clicks\")\n",
    "print(f\"  Least popular item: {item_counts.iloc[-1]:,} clicks\")\n",
    "print(f\"  Median popularity:  {item_counts.median():.0f} clicks\")\n",
    "\n",
    "# Top 20% of items account for what % of interactions?\n",
    "top_20_pct = int(n_items * 0.2)\n",
    "top_20_clicks = item_counts.iloc[:top_20_pct].sum()\n",
    "print(f\"\\n  Top 20% items ({top_20_pct} items) account for {100*top_20_clicks/n_interactions:.1f}% of clicks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize item popularity (log-log scale reveals power law)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Linear scale\n",
    "axes[0].bar(range(min(50, len(item_counts))), item_counts.values[:50], color='steelblue')\n",
    "axes[0].set_xlabel('Item Rank')\n",
    "axes[0].set_ylabel('Number of Clicks')\n",
    "axes[0].set_title('Top 50 Items by Popularity')\n",
    "\n",
    "# Log-log scale (power law appears as straight line)\n",
    "ranks = np.arange(1, len(item_counts) + 1)\n",
    "axes[1].loglog(ranks, item_counts.values, 'o', alpha=0.5, markersize=3)\n",
    "axes[1].set_xlabel('Item Rank (log)')\n",
    "axes[1].set_ylabel('Number of Clicks (log)')\n",
    "axes[1].set_title('Power Law Distribution (Log-Log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The straight line in log-log scale confirms power-law (Zipf) distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Feature Engineering / Preprocessing\n",
    "\n",
    "### 5.1 The Critical Step: Temporal Train/Test Split\n",
    "\n",
    "**Why temporal split matters:**\n",
    "\n",
    "- **Random split** = Data leakage! Future information used to predict past.\n",
    "- **Temporal split** = Realistic! Train on past, evaluate on future.\n",
    "\n",
    "```\n",
    "Timeline:  ──────────────────────────────────────────────────▶\n",
    "           [   TRAIN (80%)   ][  TEST (20%)  ]\n",
    "           Earlier sessions    Later sessions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the preprocessing function\n",
    "from preprocess_sessions import temporal_split\n",
    "\n",
    "# Perform temporal split\n",
    "train_df, test_df = temporal_split(\n",
    "    df,\n",
    "    train_ratio=CONFIG['train_ratio'],\n",
    "    filter_unseen_items=CONFIG['filter_unseen']\n",
    ")\n",
    "\n",
    "print(f\"Training set:  {len(train_df):,} interactions, {train_df['SessionId'].nunique():,} sessions\")\n",
    "print(f\"Test set:      {len(test_df):,} interactions, {test_df['SessionId'].nunique():,} sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Verifying No Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify temporal ordering\n",
    "train_max_time = train_df['Time'].max()\n",
    "test_min_time = test_df['Time'].min()\n",
    "\n",
    "print(\"TEMPORAL INTEGRITY CHECK:\")\n",
    "print(f\"  Latest training timestamp:   {train_max_time}\")\n",
    "print(f\"  Earliest test timestamp:     {test_min_time}\")\n",
    "print(f\"  Gap (seconds):               {test_min_time - train_max_time}\")\n",
    "print(f\"  No overlap:                  {train_max_time <= test_min_time}\")\n",
    "\n",
    "# Verify no session overlap\n",
    "train_sessions = set(train_df['SessionId'])\n",
    "test_sessions = set(test_df['SessionId'])\n",
    "overlap = train_sessions & test_sessions\n",
    "print(f\"  Session overlap:             {len(overlap)} (should be 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Understanding Item Filtering\n",
    "\n",
    "When `filter_unseen_items=True`, we remove items from test that never appeared in training.\n",
    "\n",
    "**Why?** We can only recommend items we've seen. Evaluating on \"impossible\" targets is misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check item overlap\n",
    "train_items = set(train_df['ItemId'])\n",
    "test_items = set(test_df['ItemId'])\n",
    "\n",
    "print(\"ITEM COVERAGE:\")\n",
    "print(f\"  Items in training:    {len(train_items)}\")\n",
    "print(f\"  Items in test:        {len(test_items)}\")\n",
    "print(f\"  Items in both:        {len(train_items & test_items)}\")\n",
    "print(f\"  Test-only items:      {len(test_items - train_items)} (filtered if enabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Session Length After Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare session length distributions\n",
    "train_lens = train_df.groupby('SessionId').size()\n",
    "test_lens = test_df.groupby('SessionId').size()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(train_lens, bins=range(2, 22), alpha=0.7, label='Train', color='steelblue')\n",
    "axes[0].axvline(train_lens.mean(), color='red', linestyle='--', label=f'Mean: {train_lens.mean():.1f}')\n",
    "axes[0].set_xlabel('Session Length')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Training Set Session Lengths')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(test_lens, bins=range(2, 22), alpha=0.7, label='Test', color='darkorange')\n",
    "axes[1].axvline(test_lens.mean(), color='red', linestyle='--', label=f'Mean: {test_lens.mean():.1f}')\n",
    "axes[1].set_xlabel('Session Length')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Test Set Session Lengths')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Core Models / Algorithms\n",
    "\n",
    "### 6.1 The Importance of Baselines\n",
    "\n",
    "Before running complex neural networks, we need baselines to answer:\n",
    "\n",
    "> \"Is the neural network actually better than simple methods?\"\n",
    "\n",
    "Surprisingly, simple baselines often achieve **60-70%** of neural network performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Baseline 1: Popularity\n",
    "\n",
    "**Algorithm:** Always recommend the globally most popular items.\n",
    "\n",
    "**Pros:**\n",
    "- Zero inference cost (precomputed list)\n",
    "- Surprisingly effective (popular items are popular for a reason)\n",
    "\n",
    "**Cons:**\n",
    "- No personalization\n",
    "- Ignores session context entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train popularity baseline\npop_baseline = PopularityBaseline()\npop_baseline.fit(train_df)\n\nprint(\"Popularity Baseline trained!\")\nprint(f\"  Learned popularity ranking for {len(pop_baseline.top_items)} items\")\nprint(f\"  Top 5 most popular items: {pop_baseline.top_items[:5]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo prediction\n",
    "example_session = [10, 25, 42]  # A user clicked items 10, 25, 42\n",
    "pop_predictions = pop_baseline.predict(example_session, k=10)\n",
    "\n",
    "print(f\"Session history: {example_session}\")\n",
    "print(f\"Top-10 recommendations: {pop_predictions}\")\n",
    "print(\"\\nNote: Recommendations are the same regardless of session (no personalization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Baseline 2: First-Order Markov Chain\n",
    "\n",
    "**Algorithm:** Recommend based on what typically follows the last clicked item.\n",
    "\n",
    "$$P(\\text{next item} | \\text{session}) \\approx P(\\text{next item} | \\text{last item})$$\n",
    "\n",
    "**Pros:**\n",
    "- Captures sequential patterns\n",
    "- Fast inference (lookup table)\n",
    "\n",
    "**Cons:**\n",
    "- Only considers last item (ignores full history)\n",
    "- Cold start for unseen items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Markov baseline\n",
    "markov_baseline = MarkovBaseline(alpha=0.0)  # Pure Markov, no popularity blending\n",
    "markov_baseline.fit(train_df)\n",
    "\n",
    "print(\"Markov Baseline trained!\")\n",
    "print(f\"  Learned transitions from {len(markov_baseline.transitions)} unique items\")\n",
    "\n",
    "# Show example transitions\n",
    "example_item = list(markov_baseline.transitions.keys())[0]\n",
    "transitions = markov_baseline.transitions[example_item]\n",
    "top_3 = sorted(transitions.items(), key=lambda x: -x[1])[:3]\n",
    "print(f\"\\nExample: After item {example_item}, users often click:\")\n",
    "for item, count in top_3:\n",
    "    print(f\"    Item {item}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo prediction - now personalized!\n",
    "example_session = [10, 25, 42]\n",
    "markov_predictions = markov_baseline.predict(example_session, k=10)\n",
    "\n",
    "print(f\"Session history: {example_session}\")\n",
    "print(f\"Last item seen: {example_session[-1]}\")\n",
    "print(f\"Top-10 recommendations: {markov_predictions}\")\n",
    "\n",
    "# Different session = different recommendations\n",
    "different_session = [100, 200, 300]\n",
    "different_preds = markov_baseline.predict(different_session, k=10)\n",
    "print(f\"\\nDifferent session {different_session[-1]} → {different_preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 GRU4Rec: The Neural Network Approach\n",
    "\n",
    "**Algorithm:** Use a Gated Recurrent Unit (GRU) to encode the full session history.\n",
    "\n",
    "```\n",
    "Session: [item1] → [item2] → [item3] → ???\n",
    "            ↓          ↓          ↓\n",
    "        ┌──────┐  ┌──────┐  ┌──────┐\n",
    "        │ GRU  │──│ GRU  │──│ GRU  │──▶ Hidden State\n",
    "        └──────┘  └──────┘  └──────┘         ↓\n",
    "                                        Score all items\n",
    "                                             ↓\n",
    "                                      Top-K recommendations\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Considers full session history\n",
    "- Learns complex sequential patterns\n",
    "\n",
    "**Cons:**\n",
    "- Requires training (time, compute)\n",
    "- More complex to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: GRU4Rec training requires the official implementation\n",
    "# For this tutorial, we focus on baselines (GRU4Rec can be trained via `make train_tiny`)\n",
    "\n",
    "print(\"GRU4Rec Model Architecture:\")\n",
    "print(\"  Input:   Session as sequence of item IDs\")\n",
    "print(\"  Layer 1: Embedding (ItemId → dense vector)\")\n",
    "print(\"  Layer 2: GRU (sequence → hidden state)\")\n",
    "print(\"  Layer 3: Output (hidden state → scores for all items)\")\n",
    "print(\"  Loss:    Cross-entropy or BPR-max\")\n",
    "print(\"\\nTo train GRU4Rec, run: make fetch && make train_tiny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Prediction / Scoring / Inference\n",
    "\n",
    "### 7.1 Understanding the Prediction Task\n",
    "\n",
    "**Next-item prediction:** Given a session prefix, predict the next item.\n",
    "\n",
    "```\n",
    "Session:     [A] → [B] → [C] → [?]\n",
    "Model sees:  [A] → [B] → [C]\n",
    "Model predicts: Ranked list of all items\n",
    "Ground truth: The actual next item the user clicked\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample session from test set\n",
    "sample_session_id = test_df['SessionId'].iloc[0]\n",
    "sample_session = test_df[test_df['SessionId'] == sample_session_id]['ItemId'].values\n",
    "\n",
    "print(f\"Sample test session {sample_session_id}:\")\n",
    "print(f\"  Full sequence: {list(sample_session)}\")\n",
    "print(f\"  Length: {len(sample_session)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate next-item prediction for each position\n",
    "print(\"Next-Item Predictions (Popularity Baseline):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for t in range(len(sample_session) - 1):\n",
    "    history = list(sample_session[:t+1])\n",
    "    target = sample_session[t+1]\n",
    "    predictions = pop_baseline.predict(history, k=10)\n",
    "    \n",
    "    hit = \"HIT\" if target in predictions else \"miss\"\n",
    "    rank = list(predictions).index(target) + 1 if target in predictions else \">10\"\n",
    "    \n",
    "    print(f\"  History: {history[-3:]:>20} → Target: {target:>4} | Rank: {rank:>4} [{hit}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Comparing Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions from both baselines\n",
    "history = list(sample_session[:-1])\n",
    "target = sample_session[-1]\n",
    "\n",
    "pop_preds = pop_baseline.predict(history, k=20)\n",
    "markov_preds = markov_baseline.predict(history, k=20)\n",
    "\n",
    "print(f\"Session history (last 5): {history[-5:]}\")\n",
    "print(f\"Target item: {target}\")\n",
    "print()\n",
    "print(f\"Popularity Top-10:  {list(pop_preds[:10])}\")\n",
    "print(f\"Markov Top-10:      {list(markov_preds[:10])}\")\n",
    "print()\n",
    "print(f\"Target in Popularity@20: {target in pop_preds}\")\n",
    "print(f\"Target in Markov@20:     {target in markov_preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Evaluation & Validation\n",
    "\n",
    "### 8.1 Metrics Explained\n",
    "\n",
    "**Recall@K:** Did the target appear in the top-K recommendations?\n",
    "$$\\text{Recall@K} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}[\\text{target}_i \\in \\text{top-K}_i]$$\n",
    "\n",
    "**MRR@K (Mean Reciprocal Rank):** How high was the target ranked?\n",
    "$$\\text{MRR@K} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\text{rank}_i} \\cdot \\mathbb{1}[\\text{rank}_i \\leq K]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate metrics with examples\n",
    "print(\"METRIC EXAMPLES:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example predictions and targets\n",
    "predictions_1 = np.array([10, 20, 30, 40, 50])  # Top-5 predictions\n",
    "target_1 = 20  # Target is at position 2\n",
    "\n",
    "predictions_2 = np.array([10, 20, 30, 40, 50])\n",
    "target_2 = 99  # Target not in predictions\n",
    "\n",
    "print(f\"\\nExample 1: predictions={list(predictions_1)}, target={target_1}\")\n",
    "print(f\"  Recall@5:  {recall_at_k(predictions_1, target_1)}  (target found)\")\n",
    "print(f\"  MRR@5:     {mrr_at_k(predictions_1, target_1):.3f}  (1/rank = 1/2 = 0.5)\")\n",
    "\n",
    "print(f\"\\nExample 2: predictions={list(predictions_2)}, target={target_2}\")\n",
    "print(f\"  Recall@5:  {recall_at_k(predictions_2, target_2)}  (target NOT found)\")\n",
    "print(f\"  MRR@5:     {mrr_at_k(predictions_2, target_2):.3f}  (rank > K → 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Full Evaluation: Why Full Ranking Matters\n",
    "\n",
    "**Sampled Evaluation (Common but Misleading):**\n",
    "- Score target against 100 random negatives\n",
    "- Fast, but inflates metrics by 2-3x!\n",
    "\n",
    "**Full Ranking Evaluation (Production-Realistic):**\n",
    "- Score target against ALL items\n",
    "- Slower, but gives realistic performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation on baselines\n",
    "print(\"Evaluating baselines (this may take a moment)...\")\n",
    "print()\n",
    "\n",
    "# Evaluate popularity baseline\n",
    "pop_results = pop_baseline.evaluate(test_df, k=CONFIG['cutoffs'])\n",
    "\n",
    "print(\"POPULARITY BASELINE RESULTS:\")\n",
    "for metric, value in pop_results.items():\n",
    "    if metric != 'n_predictions':\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "print(f\"  (Evaluated on {pop_results['n_predictions']:,} predictions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Markov baseline\n",
    "markov_results = markov_baseline.evaluate(test_df, k=CONFIG['cutoffs'])\n",
    "\n",
    "print(\"MARKOV BASELINE RESULTS:\")\n",
    "for metric, value in markov_results.items():\n",
    "    if metric != 'n_predictions':\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "print(f\"  (Evaluated on {markov_results['n_predictions']:,} predictions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': [f'Recall@{k}' for k in CONFIG['cutoffs']] + [f'MRR@{k}' for k in CONFIG['cutoffs']],\n",
    "    'Popularity': [pop_results[f'Recall@{k}'] for k in CONFIG['cutoffs']] + \n",
    "                  [pop_results[f'MRR@{k}'] for k in CONFIG['cutoffs']],\n",
    "    'Markov': [markov_results[f'Recall@{k}'] for k in CONFIG['cutoffs']] + \n",
    "              [markov_results[f'MRR@{k}'] for k in CONFIG['cutoffs']]\n",
    "})\n",
    "\n",
    "# Add winner column\n",
    "results_df['Winner'] = results_df.apply(\n",
    "    lambda row: 'Popularity' if row['Popularity'] > row['Markov'] else 'Markov', \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"BASELINE COMPARISON:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Recall comparison\n",
    "recall_data = results_df[results_df['Metric'].str.contains('Recall')]\n",
    "x = np.arange(len(recall_data))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, recall_data['Popularity'], width, label='Popularity', color='#2ecc71')\n",
    "axes[0].bar(x + width/2, recall_data['Markov'], width, label='Markov', color='#3498db')\n",
    "axes[0].set_xlabel('K')\n",
    "axes[0].set_ylabel('Recall@K')\n",
    "axes[0].set_title('Recall@K Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(CONFIG['cutoffs'])\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, max(recall_data[['Popularity', 'Markov']].max()) * 1.2)\n",
    "\n",
    "# MRR comparison\n",
    "mrr_data = results_df[results_df['Metric'].str.contains('MRR')]\n",
    "axes[1].bar(x - width/2, mrr_data['Popularity'], width, label='Popularity', color='#2ecc71')\n",
    "axes[1].bar(x + width/2, mrr_data['Markov'], width, label='Markov', color='#3498db')\n",
    "axes[1].set_xlabel('K')\n",
    "axes[1].set_ylabel('MRR@K')\n",
    "axes[1].set_title('MRR@K Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(CONFIG['cutoffs'])\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, max(mrr_data[['Popularity', 'Markov']].max()) * 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Interpreting Results\n",
    "\n",
    "**What do these numbers mean?**\n",
    "\n",
    "| Metric | Interpretation |\n",
    "|--------|---------------|\n",
    "| Recall@20 = 0.35 | 35% of the time, the item user clicked is in our top-20 recommendations |\n",
    "| MRR@20 = 0.13 | On average, the correct item appears around position 7-8 (1/0.13 ≈ 7.7) |\n",
    "\n",
    "**Is this good or bad?**\n",
    "- For production: These are baseline numbers to beat\n",
    "- Typical neural network improvement: 10-30% relative gain\n",
    "- GRU4Rec on real data typically achieves Recall@20 of 0.4-0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Outputs & Artifacts\n",
    "\n",
    "### 9.1 What the Pipeline Produces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List typical output artifacts\n",
    "artifacts = {\n",
    "    'data/': {\n",
    "        'synth_sessions.tsv': 'Full synthetic dataset',\n",
    "        'train.tsv': 'Training split (80%)',\n",
    "        'test.tsv': 'Test split (20%)'\n",
    "    },\n",
    "    'results/': {\n",
    "        'model_tiny.pt': 'Trained GRU4Rec model weights',\n",
    "        'model_tiny.config.json': 'Model configuration',\n",
    "        '*.results.json': 'Evaluation metrics'\n",
    "    },\n",
    "    'figures/': {\n",
    "        '*.png': '11 publication-quality visualizations',\n",
    "        '*.svg': 'Vector versions for papers'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"PIPELINE ARTIFACTS:\")\n",
    "print(\"=\" * 50)\n",
    "for directory, files in artifacts.items():\n",
    "    print(f\"\\n{directory}\")\n",
    "    for filename, description in files.items():\n",
    "        print(f\"  {filename:25} {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current results\n",
    "results_dir = PROJECT_ROOT / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Combine results for saving\n",
    "all_results = {\n",
    "    'Popularity': {k: v for k, v in pop_results.items() if k != 'n_predictions'},\n",
    "    'Markov': {k: v for k, v in markov_results.items() if k != 'n_predictions'}\n",
    "}\n",
    "\n",
    "results_file = results_dir / 'notebook_baselines.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")\n",
    "print(\"\\nContents:\")\n",
    "print(json.dumps(all_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 How These Outputs Are Used\n",
    "\n",
    "| Artifact | Use Case |\n",
    "|----------|----------|\n",
    "| `model.pt` | Deploy to production inference service |\n",
    "| `results.json` | Compare experiments, track progress |\n",
    "| `figures/` | Include in papers, presentations, portfolio |\n",
    "| `train/test.tsv` | Reproduce experiments, debugging |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. How to Modify & Experiment\n",
    "\n",
    "### 10.1 Configuration Changes\n",
    "\n",
    "The `CONFIG` dictionary at the top controls key parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show modifiable configuration\n",
    "print(\"MODIFIABLE CONFIGURATION:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Data Generation:\")\n",
    "print(\"  CONFIG['n_sessions'] = 1000  # Increase for more realistic data\")\n",
    "print(\"  CONFIG['n_items'] = 500      # Increase for harder task\")\n",
    "print()\n",
    "print(\"Data Split:\")\n",
    "print(\"  CONFIG['train_ratio'] = 0.8  # Try 0.9 for more training data\")\n",
    "print(\"  CONFIG['filter_unseen'] = True  # Try False to see impact\")\n",
    "print()\n",
    "print(\"Evaluation:\")\n",
    "print(\"  CONFIG['cutoffs'] = [5, 10, 20]  # Add [1, 3, 50] for more granularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Model Parameter Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov baseline has a tunable parameter\n",
    "print(\"MARKOV BASELINE TUNING:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"alpha parameter controls popularity blending:\")\n",
    "print(\"  alpha=0.0  → Pure Markov (transition probabilities only)\")\n",
    "print(\"  alpha=0.5  → 50% Markov, 50% popularity\")\n",
    "print(\"  alpha=1.0  → Pure popularity (ignores transitions)\")\n",
    "print()\n",
    "print(\"Example: MarkovBaseline(alpha=0.3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick experiment: Compare different alpha values\n",
    "print(\"Quick Experiment: Markov alpha sweep\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for alpha in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    model = MarkovBaseline(alpha=alpha)\n",
    "    model.fit(train_df)\n",
    "    results = model.evaluate(test_df, k=[20])\n",
    "    print(f\"  alpha={alpha:.2f} → Recall@20: {results['Recall@20']:.4f}, MRR@20: {results['MRR@20']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 GRU4Rec Parameter Changes\n",
    "\n",
    "If you want to experiment with GRU4Rec itself, modify these in `scripts/run_gru4rec.py` or pass as command-line arguments:\n",
    "\n",
    "```bash\n",
    "# Larger model\n",
    "python scripts/run_gru4rec.py train --layers 256 --epochs 20\n",
    "\n",
    "# Different loss function\n",
    "python scripts/run_gru4rec.py train --loss bpr-max\n",
    "\n",
    "# GPU training\n",
    "python scripts/run_gru4rec.py train --device cuda:0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Guided Exercises\n",
    "\n",
    "These exercises will deepen your understanding of the system. Work through them to build intuition.\n",
    "\n",
    "### Exercise 1: Impact of Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1: How does dataset size affect performance?\n",
    "# \n",
    "# WHAT TO DO: Generate datasets of different sizes and compare performance\n",
    "# WHAT TO OBSERVE: Does more data always help? Diminishing returns?\n",
    "# WHY IT MATTERS: Determines data collection requirements for production\n",
    "\n",
    "print(\"EXERCISE 1: Dataset Size Impact\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for n_sess in [100, 500, 1000, 2000]:\n",
    "    # Generate data\n",
    "    df_exp = generate_synthetic_sessions(n_sessions=n_sess, n_items=500, seed=42)\n",
    "    train_exp, test_exp = temporal_split(df_exp, train_ratio=0.8)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    pop = PopularityBaseline().fit(train_exp)\n",
    "    results = pop.evaluate(test_exp, k=[20])\n",
    "    \n",
    "    print(f\"  n_sessions={n_sess:5} → Recall@20: {results['Recall@20']:.4f}\")\n",
    "\n",
    "print(\"\\nObservation: _____________________________________\")\n",
    "print(\"(Fill in what you notice about the trend)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Train/Test Ratio Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2: How does train/test split ratio affect results?\n",
    "#\n",
    "# WHAT TO DO: Try different train_ratio values\n",
    "# WHAT TO OBSERVE: Performance vs statistical reliability tradeoff\n",
    "# WHY IT MATTERS: Affects confidence in your evaluation numbers\n",
    "\n",
    "print(\"EXERCISE 2: Train/Test Ratio Impact\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the original full dataset\n",
    "for ratio in [0.5, 0.7, 0.8, 0.9]:\n",
    "    train_exp, test_exp = temporal_split(df, train_ratio=ratio)\n",
    "    \n",
    "    pop = PopularityBaseline().fit(train_exp)\n",
    "    results = pop.evaluate(test_exp, k=[20])\n",
    "    \n",
    "    print(f\"  ratio={ratio:.1f} → Train: {len(train_exp):5}, Test: {len(test_exp):4} | Recall@20: {results['Recall@20']:.4f}\")\n",
    "\n",
    "print(\"\\nObservation: _____________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Item Filtering Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 3: What happens if we don't filter unseen items?\n",
    "#\n",
    "# WHAT TO DO: Compare with and without filter_unseen_items\n",
    "# WHAT TO OBSERVE: How much does filtering affect metrics?\n",
    "# WHY IT MATTERS: Determines if your evaluation is realistic\n",
    "\n",
    "print(\"EXERCISE 3: Item Filtering Impact\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# With filtering (default)\n",
    "train_filtered, test_filtered = temporal_split(df, filter_unseen_items=True)\n",
    "pop_filtered = PopularityBaseline().fit(train_filtered)\n",
    "results_filtered = pop_filtered.evaluate(test_filtered, k=[20])\n",
    "\n",
    "# Without filtering\n",
    "train_unfiltered, test_unfiltered = temporal_split(df, filter_unseen_items=False)\n",
    "pop_unfiltered = PopularityBaseline().fit(train_unfiltered)\n",
    "results_unfiltered = pop_unfiltered.evaluate(test_unfiltered, k=[20])\n",
    "\n",
    "print(f\"  With filtering:    Recall@20 = {results_filtered['Recall@20']:.4f} (test size: {len(test_filtered)})\")\n",
    "print(f\"  Without filtering: Recall@20 = {results_unfiltered['Recall@20']:.4f} (test size: {len(test_unfiltered)})\")\n",
    "\n",
    "print(\"\\nQuestion: Why is the unfiltered Recall@20 lower?\")\n",
    "print(\"Answer: _____________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Catalog Size Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 4: How does catalog size affect difficulty?\n",
    "#\n",
    "# WHAT TO DO: Generate data with different n_items values\n",
    "# WHAT TO OBSERVE: Harder task with more items?\n",
    "# WHY IT MATTERS: Real catalogs have 10K-1M items\n",
    "\n",
    "print(\"EXERCISE 4: Catalog Size Impact\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for n_items in [100, 500, 1000, 2000]:\n",
    "    df_exp = generate_synthetic_sessions(n_sessions=1000, n_items=n_items, seed=42)\n",
    "    train_exp, test_exp = temporal_split(df_exp)\n",
    "    \n",
    "    pop = PopularityBaseline().fit(train_exp)\n",
    "    results = pop.evaluate(test_exp, k=[20])\n",
    "    \n",
    "    print(f\"  n_items={n_items:5} → Recall@20: {results['Recall@20']:.4f}\")\n",
    "\n",
    "print(\"\\nObservation: More items makes the task _____________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Markov Alpha Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 5: Find the optimal alpha for Markov baseline\n",
    "#\n",
    "# WHAT TO DO: Sweep alpha from 0 to 1 in small increments\n",
    "# WHAT TO OBSERVE: Is there a sweet spot?\n",
    "# WHY IT MATTERS: Shows value of blending approaches\n",
    "\n",
    "print(\"EXERCISE 5: Optimal Markov Alpha\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "alphas = np.linspace(0, 1, 11)\n",
    "recall_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = MarkovBaseline(alpha=alpha)\n",
    "    model.fit(train_df)\n",
    "    results = model.evaluate(test_df, k=[20])\n",
    "    recall_scores.append(results['Recall@20'])\n",
    "\n",
    "# Find best\n",
    "best_idx = np.argmax(recall_scores)\n",
    "print(f\"  Best alpha: {alphas[best_idx]:.2f} with Recall@20: {recall_scores[best_idx]:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(alphas, recall_scores, 'o-', linewidth=2, markersize=8)\n",
    "plt.axvline(alphas[best_idx], color='red', linestyle='--', label=f'Best: {alphas[best_idx]:.2f}')\n",
    "plt.xlabel('Alpha (0=Markov, 1=Popularity)')\n",
    "plt.ylabel('Recall@20')\n",
    "plt.title('Markov-Popularity Blend Optimization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Session Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 6: Does performance vary by session length?\n",
    "#\n",
    "# WHAT TO DO: Segment test data by session length and evaluate separately\n",
    "# WHAT TO OBSERVE: Are longer sessions easier to predict?\n",
    "# WHY IT MATTERS: Informs whether to prioritize short vs long sessions\n",
    "\n",
    "print(\"EXERCISE 6: Performance by Session Length\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group sessions by length\n",
    "test_session_lens = test_df.groupby('SessionId').size()\n",
    "\n",
    "# Define length buckets\n",
    "buckets = [(2, 4), (5, 8), (9, 15), (16, 20)]\n",
    "\n",
    "for min_len, max_len in buckets:\n",
    "    # Filter sessions in this length range\n",
    "    valid_sessions = test_session_lens[(test_session_lens >= min_len) & (test_session_lens <= max_len)].index\n",
    "    if len(valid_sessions) == 0:\n",
    "        continue\n",
    "    test_subset = test_df[test_df['SessionId'].isin(valid_sessions)]\n",
    "    \n",
    "    # Evaluate on subset\n",
    "    results = pop_baseline.evaluate(test_subset, k=[20])\n",
    "    \n",
    "    print(f\"  Length {min_len}-{max_len}: {len(valid_sessions):4} sessions → Recall@20: {results['Recall@20']:.4f}\")\n",
    "\n",
    "print(\"\\nObservation: _____________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Limitations & Next Steps\n",
    "\n",
    "### 12.1 What This Project Does NOT Handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limitations = [\n",
    "    (\"Real-time inference\", \"No serving infrastructure; batch evaluation only\"),\n",
    "    (\"Item features\", \"Only uses item IDs; no product metadata, images, text\"),\n",
    "    (\"User features\", \"Purely session-based; no demographics or history\"),\n",
    "    (\"Multi-objective\", \"Only click prediction; no revenue/diversity optimization\"),\n",
    "    (\"A/B testing\", \"Offline evaluation only; no online experimentation\"),\n",
    "    (\"Scale\", \"Tested on 1K sessions; real systems have millions\"),\n",
    "]\n",
    "\n",
    "print(\"KNOWN LIMITATIONS:\")\n",
    "print(\"=\" * 60)\n",
    "for limitation, description in limitations:\n",
    "    print(f\"\\n  {limitation}\")\n",
    "    print(f\"    → {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Logical Next Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = [\n",
    "    (\"Real data\", \"RetailRocket, RecSys Challenge datasets\", \"High\"),\n",
    "    (\"More baselines\", \"SKNN, STAN, SASRec\", \"Medium\"),\n",
    "    (\"Hyperparameter tuning\", \"Optuna integration for GRU4Rec\", \"Medium\"),\n",
    "    (\"Item features\", \"Incorporate product categories, prices\", \"Medium\"),\n",
    "    (\"Serving API\", \"FastAPI endpoint for real-time inference\", \"High\"),\n",
    "    (\"MLflow tracking\", \"Experiment tracking and model registry\", \"Low\"),\n",
    "]\n",
    "\n",
    "print(\"POSSIBLE EXTENSIONS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Extension':<25} {'Description':<35} {'Priority'}\")\n",
    "print(\"-\" * 60)\n",
    "for ext, desc, priority in extensions:\n",
    "    print(f\"{ext:<25} {desc:<35} {priority}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 Quick Reference Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "QUICK REFERENCE - Common Operations:\n",
    "================================================================================\n",
    "\n",
    "# Run full demo pipeline (30 seconds)\n",
    "make demo\n",
    "\n",
    "# Generate larger synthetic dataset\n",
    "python scripts/make_synth_data.py --n_sessions 10000 --n_items 1000\n",
    "\n",
    "# Train GRU4Rec (requires GPU for speed)\n",
    "make fetch\n",
    "python scripts/run_gru4rec.py train --device cuda:0 --epochs 10\n",
    "\n",
    "# Generate all visualizations\n",
    "make visualize\n",
    "\n",
    "# Run tests\n",
    "make test\n",
    "\n",
    "# Clean all generated files\n",
    "make clean\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you've learned:\n",
    "\n",
    "1. **The Problem**: Session-based recommendations for anonymous users\n",
    "2. **The Pipeline**: Data → Split → Train → Evaluate (with temporal integrity)\n",
    "3. **The Baselines**: Popularity and Markov Chain as competitive benchmarks\n",
    "4. **The Metrics**: Recall@K and MRR@K with full ranking evaluation\n",
    "5. **The Experiments**: How to modify parameters and observe effects\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Simple baselines are surprisingly strong (60-70% of neural networks)\n",
    "- Temporal splits prevent data leakage\n",
    "- Full ranking evaluation gives realistic estimates\n",
    "- Reproducibility requires discipline (seeds, versions, documentation)\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook is part of the GRU4Rec Reproduction Study portfolio project.*\n",
    "\n",
    "*Repository: https://github.com/oscgonz19/gru4rec-reproduction-and-audit*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}